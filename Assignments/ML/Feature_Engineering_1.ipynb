{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a00fc68-6754-4931-80dc-f57db942e41d",
   "metadata": {},
   "source": [
    "Q1. What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda2ffa-3b1b-40fc-9cf1-4af32e528f30",
   "metadata": {},
   "source": [
    "Missing values refer to the absence of a particular data point in a dataset. In other words, it is the lack of information about a particular variable in a certain observation. Missing values can occur due to various reasons such as data entry errors, equipment malfunction, non-response, or simply a lack of information.\n",
    "\n",
    "Handling missing values is essential because they can negatively impact the quality of the analysis or model that is built from the data. Missing values can lead to biased estimates, reduced statistical power, and inaccurate predictions.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "1. Decision Trees: Decision trees can handle missing values by simply creating a new branch in the tree for the missing values.\n",
    "\n",
    "2. Random Forests: Random forests can handle missing values by using the available data to estimate missing values and then using this imputed data to build the trees.\n",
    "\n",
    "3. K-Nearest Neighbors (KNN): KNN can handle missing values by simply ignoring the missing values when computing distances between observations.\n",
    "\n",
    "4. Support Vector Machines (SVM): SVM can handle missing values by imputing them with the mean or median of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bbe55-ed7b-4520-8cfd-31546a19a7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "855ee76e-f653-42b5-a837-8b06d085add3",
   "metadata": {},
   "source": [
    "Q2. List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0f9b4-c09f-441c-9ad9-9fd2a300a64c",
   "metadata": {},
   "source": [
    "Here are some techniques that can be used to handle missing data:\n",
    "\n",
    "Deletion: This technique involves removing the observations or variables with missing data. There are two types of deletion:\n",
    "\n",
    "1. Listwise Deletion: This involves deleting entire rows of data that contain missing values.\n",
    "\n",
    "2. Pairwise Deletion: This involves deleting only the missing values themselves, rather than entire rows.\n",
    "\n",
    "Imputation: This technique involves filling in the missing values with estimated values. There are several methods of imputation:\n",
    "\n",
    "1. Mean Imputation: This involves filling in the missing values with the mean of the available data.\n",
    "\n",
    "2. Median Imputation: This involves filling in the missing values with the median of the available data.\n",
    "\n",
    "3. Mode Imputation: This involves filling in the missing values with the mode of the available data.\n",
    "\n",
    "Interpolation: This technique involves filling in the missing values by interpolating between the available data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a468f74-ac98-4de1-b879-9b446a16c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "#Listwise Deletion\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "# remove rows with missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01aab9e-8bcf-45cc-807e-d0870c8b3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "#Mean Imputation\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "\n",
    "# impute missing values with mean\n",
    "df_clean = df.fillna(df.mean())\n",
    "\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1d8da4-4d95-4ab2-b005-63eb0b0d5a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "# interpolate missing values using linear interpolation\n",
    "df_clean = df.interpolate()\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa407d7-a8b4-4908-a0ac-e3909803c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4abb0e2-4da5-4fae-ad70-6904a2340c9d",
   "metadata": {},
   "source": [
    "Q3. Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a760b0-8afe-4439-bcf5-35ed6b9bc063",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in which the number of observations in each class of a classification problem is not equally represented. In other words, one class may have significantly more samples than the other classes.\n",
    "\n",
    "For example, if we are trying to predict whether a credit card transaction is fraudulent or not, the number of non-fraudulent transactions will likely be much higher than the number of fraudulent transactions.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased or inaccurate models. Some metrics like accuracy can be misleading when the data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb311d-823e-4e26-83bb-5e56fb938f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab739dd9-1c10-479f-950c-31773dfddbf5",
   "metadata": {},
   "source": [
    "Q4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c5098-8379-48e2-ba5d-f8136d8d8782",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced data.\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class to balance the class distribution. This can be done by duplicating existing samples or generating new synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Down-sampling involves decreasing the number of samples in the majority class to balance the class distribution. This can be done by randomly removing samples from the majority class until the desired class distribution is achieved.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we have a dataset of credit card transactions, where 98% of the transactions are legitimate and 2% are fraudulent. We want to build a classification model to detect fraudulent transactions. If we train our model on this imbalanced dataset, it is likely that the model will be biased towards the majority class and will not accurately predict the minority class. In this case, we may want to consider up-sampling the minority class to balance the class distribution. This can be done by generating synthetic samples of fraudulent transactions using techniques like SMOTE, which can improve the performance of the model on the minority class.\n",
    "\n",
    "On the other hand, in some cases, we may have a dataset with too many samples in the minority class. \n",
    "\n",
    "For example, in a medical diagnosis problem, the number of positive cases may be much smaller than the number of negative cases. In this case, we may want to consider down-sampling the majority class to balance the class distribution. This can help to prevent the model from being overly biased towards the majority class and improve its ability to predict the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1134ace-c820-4084-b97c-850154695641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de342965-d1b9-4580-8a77-29d92c83958f",
   "metadata": {},
   "source": [
    "Q5. What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c8daa-442f-49a3-8962-dc8e554efd27",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by generating new samples from existing data. It is commonly used in machine learning to overcome the problem of limited training data.\n",
    "\n",
    "Data augmentation can involve various operations such as flipping, rotating, scaling, adding noise, or changing the color of images. By generating new data from existing data, data augmentation can help to reduce overfitting and improve the generalization performance of the model.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to address the problem of imbalanced data. SMOTE is designed to generate synthetic samples of the minority class by interpolating between existing samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f310c59-2f40-4983-8f04-2bb0b5f7b659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4ef301-22b3-4ee4-b66f-87d94a79d685",
   "metadata": {},
   "source": [
    "Q6. What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51c23a-a421-4f3e-a8cc-380e4cdda918",
   "metadata": {},
   "source": [
    "Outliers are data points that deviate significantly from the rest of the dataset. They can be caused by various factors such as measurement errors, data processing errors, or simply being a rare occurrence.\n",
    "\n",
    "Outliers can affect the distribution of the data, making it difficult to obtain accurate summary statistics such as the mean and standard deviation. They can also influence the results of machine learning algorithms, leading to inaccurate predictions or overfitting. Outliers can also distort data visualization, making it difficult to observe patterns in the data.\n",
    "\n",
    "Therefore, It is essential to handle outliers because they can lead to inaccurate conclusions and decisions based on the data. Handling outliers can improve the accuracy of statistical analysis and machine learning models and improve the interpretability of data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00736b-4b92-4786-8f9a-d3e01d83b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "995a941d-09b1-4d75-a50c-224a22957e99",
   "metadata": {},
   "source": [
    "Q7. You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a84fb5-aec5-4b74-b8e4-6aee61c30b66",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis. Some of the common techniques include:\n",
    "\n",
    "1. Deletion: In this technique, we simply delete the rows or columns that contain missing values. This can be useful if the missing data is small in number and doesn't have a significant impact on the overall analysis. \n",
    "\n",
    "2. Mean/median/mode imputation: In this technique, we replace the missing value with the mean, median, or mode of the non-missing values in the same column. This method can be useful if the missing values are random and the distribution of the non-missing values is not significantly affected by the missing data.\n",
    "\n",
    "3. Regression imputation: In this technique, we use regression analysis to predict the missing value based on the values of other variables. This method can be useful if there is a strong correlation between the missing value and other variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba922b-bb2b-493f-a788-0a8647cbf66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28cac3f1-a840-473a-af06-b628c43b061a",
   "metadata": {},
   "source": [
    "Q8. You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0b71b-419c-49ec-a137-b193ea0cf683",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random (MAR) or if there is a pattern to the missing data. Some of the common strategies include:\n",
    "\n",
    "1. Visual inspection: Plotting the data and visually inspecting the pattern of missing data can provide some insight into whether the missing data is random or not.\n",
    "\n",
    "2. Correlation analysis: Examining the correlation between the missing values and other variables in the dataset can provide some insight into whether the missing data is related to other variables. If there is a significant correlation between the missing values and other variables, it may indicate that the missing data is non-random.\n",
    "\n",
    "3. Statistical tests: Performing statistical tests such as the chi-squared test or t-test can help determine whether the missing data is random or not. For example, the chi-squared test can be used to test whether the pattern of missing data is different from what would be expected by chance.\n",
    "\n",
    "4. Imputation techniques: Applying different imputation techniques such as mean imputation, regression imputation, or K-nearest neighbor imputation can help determine the effectiveness of the imputation technique in handling the missing data. If the imputation technique performs well, it may indicate that the missing data is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9094a72-fdd8-43f0-8576-574c009c6e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b8123c-6d18-4594-8aaa-7a68abe26d4b",
   "metadata": {},
   "source": [
    "Q9. Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f3fef-8f21-462f-a876-bdf6dccb2498",
   "metadata": {},
   "source": [
    "Some of the common strategies include:\n",
    "\n",
    "1. Confusion matrix: A confusion matrix is a table that summarizes the performance of a machine learning model on a dataset. It shows the number of true positives, true negatives, false positives, and false negatives. This can be useful in evaluating the sensitivity, specificity, precision, recall, and accuracy of the model.\n",
    "\n",
    "2. Precision-Recall curve: The precision-recall curve is a graphical representation of the performance of a machine learning model. It shows the trade-off between precision and recall at different threshold values. This can be useful in evaluating the overall performance of the model and selecting an appropriate threshold value.\n",
    "\n",
    "3. ROC curve: The ROC curve is a graphical representation of the performance of a machine learning model. It shows the trade-off between sensitivity and specificity at different threshold values. This can be useful in evaluating the overall performance of the model and selecting an appropriate threshold value.\n",
    "\n",
    "4. F1-score: The F1-score is a metric that combines precision and recall into a single value. It can be useful in evaluating the overall performance of a machine learning model on an imbalanced dataset.\n",
    "\n",
    "5. Stratified sampling: In stratified sampling, we ensure that the proportion of positive and negative samples is the same in the training and testing datasets. This can help ensure that the model is evaluated on a balanced dataset.\n",
    "\n",
    "6. Resampling techniques: Resampling techniques such as oversampling or undersampling can be used to balance the dataset. Oversampling involves increasing the number of positive samples in the dataset, while undersampling involves reducing the number of negative samples in the dataset. These techniques can be useful in improving the performance of the machine learning model on the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca9bb7-2b1e-42fe-9b90-5a8b032dd99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0457ad7a-f2d2-4e3e-a9b2-731e7b02da35",
   "metadata": {},
   "source": [
    "Q10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c870b-1a66-44f4-86c5-cb34e059fbba",
   "metadata": {},
   "source": [
    "The Methods we can employ are:\n",
    "\n",
    "1. Random undersampling: In this technique, a random sample of data from the majority class is selected and removed from the dataset. This method can be effective when the dataset is very large, and the majority class is significantly larger than the minority class.\n",
    "\n",
    "2. Tomek links: Tomek links are pairs of data points that are close to each other but belong to different classes. In this technique, the majority class examples that are close to the minority class examples are removed from the dataset. This technique can be useful in preserving the structure of the minority class and improving the classification performance.\n",
    "\n",
    "3. Cluster centroids: In this technique, the centroids of the clusters of the majority class are calculated, and then only those examples from the majority class that are closest to the centroids are kept in the dataset. This method can be effective when the majority class is very large and clustered.\n",
    "\n",
    "4. Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that generates synthetic examples of the minority class to balance the dataset. It creates synthetic examples by interpolating between the minority class examples. This method can be effective when the minority class is small, and the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a526962-08a2-4a32-bd7b-4546a1a164ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f648311-4e1e-400b-b77e-59e9e84bd99f",
   "metadata": {},
   "source": [
    "Q11. You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095acaa1-5b54-45ed-8c1b-c26f489fa6f8",
   "metadata": {},
   "source": [
    "We can use:\n",
    "\n",
    "1. Random oversampling: In this technique, the minority class is randomly duplicated to increase its size to the same size as the majority class. This method can be effective when the dataset is very small, and the minority class is significantly smaller than the majority class.\n",
    "\n",
    "2. Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that generates synthetic examples of the minority class to balance the dataset. It creates synthetic examples by interpolating between the minority class examples. This method can be effective when the minority class is small, and the dataset is imbalanced.\n",
    "\n",
    "3. Adaptive Synthetic Sampling (ADASYN): ADASYN is a technique that generates synthetic examples of the minority class based on the density distribution of the data. This method is effective when there is a significant overlap between the minority and majority classes.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique with Iterative Refinement (SMOTE-IR): SMOTE-IR is an extension of SMOTE that generates synthetic examples iteratively and refines the synthetic examples using an SVM classifier. This method is effective when the dataset is highly imbalanced, and the minority class is significantly smaller than the majority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
